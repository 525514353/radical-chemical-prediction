{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-06-12T15:30:03.201245Z",
     "end_time": "2025-06-12T15:30:21.340691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDKit found. Advanced SMILES processing, similarity, and property calculation will be used.\n",
      "Using device: cuda\n",
      "Loading tokenizer from ./best_model_multi_eval_v3_correct_loss/...\n",
      "Loading base model 'google/flan-t5-base' and adapter from './best_model_multi_eval_v3_correct_loss/'...\n",
      "Model loaded successfully.\n",
      "Loaded true products map for potential matching.\n",
      "\n",
      "--- Predicting for Reactant: C=CC(C)C.[OH] ---\n",
      "\n",
      "--- Original Model Ranking ---\n",
      "  Rank 1: [CH2]C(O)C(C)C\n",
      "  Rank 2: CC(C)[CH]CO\n",
      "  Rank 3: C=CC(C)C.O\n",
      "  Rank 4: )(C)C.O\n",
      "  Rank 5: CC[CH]C(C)C.O\n",
      "\n",
      "--- Comprehensive Score Reranking ---\n",
      "  Reranked 1 (Orig Rank 1):\n",
      "    Prediction: [CH2]C(O)C(C)C\n",
      "    Model Score: 1.000, Validity: 1.000, Balance: 1.000\n",
      "    Comprehensive Score: 1.0000\n",
      "    Matches GT: True\n",
      "    Max Tanimoto: 1.000\n",
      "  Reranked 2 (Orig Rank 2):\n",
      "    Prediction: CC(C)[CH]CO\n",
      "    Model Score: 0.800, Validity: 1.000, Balance: 1.000\n",
      "    Comprehensive Score: 0.9200\n",
      "    Matches GT: True\n",
      "    Max Tanimoto: 1.000\n",
      "  Reranked 3 (Orig Rank 3):\n",
      "    Prediction: C=CC(C)C.O\n",
      "    Model Score: 0.600, Validity: 1.000, Balance: 1.000\n",
      "    Comprehensive Score: 0.8400\n",
      "    Matches GT: False\n",
      "    Max Tanimoto: 0.278\n",
      "  Reranked 4 (Orig Rank 5):\n",
      "    Prediction: CC[CH]C(C)C.O\n",
      "    Model Score: 0.200, Validity: 1.000, Balance: 0.833\n",
      "    Comprehensive Score: 0.6300\n",
      "    Matches GT: False\n",
      "    Max Tanimoto: 0.444\n",
      "  Reranked 5 (Orig Rank 4):\n",
      "    Prediction: )(C)C.O\n",
      "    Model Score: 0.400, Validity: 0.100, Balance: 0.500\n",
      "    Comprehensive Score: 0.3400\n",
      "    Matches GT: False\n",
      "    Max Tanimoto: 0.000\n",
      "\n",
      "--- Accuracy Comparison ---\n",
      "True Products: CC(C)[CH]CO; [CH2]C(O)C(C)C\n",
      "\n",
      "Original Model Ranking:\n",
      "  Top-1: ✓\n",
      "  Top-3: ✓\n",
      "  Top-5: ✓\n",
      "  Top-10: ✓\n",
      "\n",
      "Comprehensive Score Reranking:\n",
      "  Top-1: ✓\n",
      "  Top-3: ✓\n",
      "  Top-5: ✓\n",
      "  Top-10: ✓\n",
      "\n",
      "Improvement:\n",
      "  TOP1: =\n",
      "  TOP3: =\n",
      "  TOP5: =\n",
      "  TOP10: =\n",
      "\n",
      "Detailed predictions saved to ./single_reactant_predictions_comprehensive_scores.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "import numpy as np\n",
    "import traceback\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Attempt to import RDKit\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit import RDLogger\n",
    "    from rdkit.DataStructs import TanimotoSimilarity\n",
    "    from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect # ECFP\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    RDKIT_AVAILABLE = True\n",
    "    print(\"RDKit found. Advanced SMILES processing, similarity, and property calculation will be used.\")\n",
    "except ImportError:\n",
    "    RDKIT_AVAILABLE = False\n",
    "    print(\"RDKit not found. SMILES processing will be string-based, no fingerprint or advanced properties.\")\n",
    "    print(\"For better accuracy and features, please install RDKit: pip install rdkit-pypi\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_NAME = \"google/flan-t5-base\"\n",
    "ADAPTER_MODEL_PATH = \"./best_model_multi_eval_v3_correct_loss/\"\n",
    "DATA_PATH = './data/data'\n",
    "OUTPUT_CSV_PATH_SINGLE_FORMATTED = \"./single_reactant_predictions_comprehensive_scores.csv\"\n",
    "\n",
    "INPUT_REACTANT_SMILES = \"C=CC(C)C.[OH]\" # REPLACE THIS or set to None\n",
    "LOAD_FROM_TEST_SET_INDEX = 0\n",
    "\n",
    "NUM_RETURN_SEQUENCES = 5\n",
    "MAX_LENGTH_GENERATION = 256\n",
    "NUM_BEAMS = NUM_RETURN_SEQUENCES*2\n",
    "TEMPERATURE = 1.0\n",
    "TOP_K = 50\n",
    "TOP_P = 0.95\n",
    "DO_SAMPLE = False\n",
    "\n",
    "# Scoring weights for comprehensive score\n",
    "MODEL_SCORE_WEIGHT = 0.4    # 模型排名权重\n",
    "VALIDITY_SCORE_WEIGHT = 0.3 # 化学有效性权重\n",
    "BALANCE_SCORE_WEIGHT = 0.3  # 原子平衡权重\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "FINGERPRINT_SIMILARITY_THRESHOLD = 1\n",
    "\n",
    "# --- Atom Balancing Functions ---\n",
    "def get_atom_counts_from_smiles(smiles: str) -> Counter:\n",
    "    \"\"\"\n",
    "    Extract atom counts from SMILES string using RDKit when available,\n",
    "    with fallback to regex-based parsing for cases with radicals or abbreviations.\n",
    "    \"\"\"\n",
    "    if not smiles or not isinstance(smiles, str):\n",
    "        return Counter()\n",
    "\n",
    "    smiles = smiles.strip()\n",
    "    if not smiles:\n",
    "        return Counter()\n",
    "\n",
    "    # First try RDKit if available\n",
    "    if RDKIT_AVAILABLE:\n",
    "        try:\n",
    "            # Try parsing with sanitization\n",
    "            mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "            if mol:\n",
    "                atom_counts = Counter()\n",
    "                for atom in mol.GetAtoms():\n",
    "                    symbol = atom.GetSymbol()\n",
    "                    atom_counts[symbol] += 1\n",
    "                return atom_counts\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Try parsing without sanitization for radicals\n",
    "            mol = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "            if mol:\n",
    "                atom_counts = Counter()\n",
    "                for atom in mol.GetAtoms():\n",
    "                    symbol = atom.GetSymbol()\n",
    "                    atom_counts[symbol] += 1\n",
    "                return atom_counts\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback to regex-based parsing\n",
    "    return _regex_atom_count_fallback(smiles)\n",
    "\n",
    "def _regex_atom_count_fallback(smiles: str) -> Counter:\n",
    "    \"\"\"\n",
    "    Fallback method to extract atom counts using regex patterns.\n",
    "    Handles basic SMILES notation including some radical notations.\n",
    "    \"\"\"\n",
    "    atom_counts = Counter()\n",
    "\n",
    "    # Remove brackets for simplicity in counting, but keep track of charges and radicals\n",
    "    # Pattern to match atoms (including those in brackets)\n",
    "    # This pattern matches: [Element], [Element+], [Element-], [Element], Element\n",
    "    atom_pattern = r'\\[([A-Z][a-z]?)(?:[+-]?\\d*|\\.)*\\]|([A-Z][a-z]?)'\n",
    "\n",
    "    matches = re.findall(atom_pattern, smiles)\n",
    "\n",
    "    for match in matches:\n",
    "        # match is a tuple: (bracketed_atom, unbracketted_atom)\n",
    "        atom = match[0] if match[0] else match[1]\n",
    "        if atom:\n",
    "            # Handle common abbreviations\n",
    "            atom = _expand_atom_abbreviation(atom)\n",
    "            atom_counts[atom] += 1\n",
    "\n",
    "    # Handle explicit hydrogen counts in brackets like [CH3], [NH2], etc.\n",
    "    bracket_pattern = r'\\[([A-Z][a-z]?)H?(\\d*)[+-]?\\d*\\.?\\]'\n",
    "    bracket_matches = re.findall(bracket_pattern, smiles)\n",
    "\n",
    "    for atom, h_count in bracket_matches:\n",
    "        if h_count:\n",
    "            try:\n",
    "                h_num = int(h_count) if h_count else 1\n",
    "                atom_counts['H'] += h_num\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    return atom_counts\n",
    "\n",
    "def _expand_atom_abbreviation(atom: str) -> str:\n",
    "    \"\"\"\n",
    "    Expand common atom abbreviations to standard element symbols.\n",
    "    \"\"\"\n",
    "    abbreviations = {\n",
    "        'Me': 'C',  # Methyl - simplified to carbon\n",
    "        'Et': 'C',  # Ethyl - simplified to carbon\n",
    "        'Ph': 'C',  # Phenyl - simplified to carbon\n",
    "        'Ac': 'C',  # Acetyl - simplified to carbon\n",
    "        'Bn': 'C',  # Benzyl - simplified to carbon\n",
    "        'Bu': 'C',  # Butyl - simplified to carbon\n",
    "        'Pr': 'C',  # Propyl - simplified to carbon\n",
    "        'Tf': 'C',  # Trifluoromethyl - simplified (should include F but simplified here)\n",
    "        'Ts': 'C',  # Tosyl - simplified to carbon\n",
    "        'Boc': 'C', # tert-Butoxycarbonyl - simplified to carbon\n",
    "        'Cbz': 'C', # Carboxybenzyl - simplified to carbon\n",
    "    }\n",
    "    return abbreviations.get(atom, atom)\n",
    "\n",
    "def parse_multi_component_smiles(multi_smiles: str) -> list:\n",
    "    \"\"\"\n",
    "    Parse multi-component SMILES (separated by '.') into individual components.\n",
    "    \"\"\"\n",
    "    if not multi_smiles or not isinstance(multi_smiles, str):\n",
    "        return []\n",
    "\n",
    "    # Split by '.' to get individual components\n",
    "    components = [comp.strip() for comp in multi_smiles.split('.') if comp.strip()]\n",
    "    return components\n",
    "\n",
    "def check_atom_balance(reactant_smiles: str, product_smiles: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if the reaction is atom-balanced between reactants and products.\n",
    "\n",
    "    Args:\n",
    "        reactant_smiles: SMILES string of reactants (may be multi-component)\n",
    "        product_smiles: SMILES string of products (may be multi-component)\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'is_balanced': bool,\n",
    "            'reactant_atoms': Counter,\n",
    "            'product_atoms': Counter,\n",
    "            'missing_in_products': Counter,\n",
    "            'extra_in_products': Counter,\n",
    "            'balance_score': float  # 0.0 to 1.0, where 1.0 is perfectly balanced\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Parse multi-component SMILES\n",
    "    reactant_components = parse_multi_component_smiles(reactant_smiles)\n",
    "    product_components = parse_multi_component_smiles(product_smiles)\n",
    "\n",
    "    # Count atoms in reactants\n",
    "    reactant_atoms = Counter()\n",
    "    for component in reactant_components:\n",
    "        component_atoms = get_atom_counts_from_smiles(component)\n",
    "        reactant_atoms.update(component_atoms)\n",
    "\n",
    "    # Count atoms in products\n",
    "    product_atoms = Counter()\n",
    "    for component in product_components:\n",
    "        component_atoms = get_atom_counts_from_smiles(component)\n",
    "        product_atoms.update(component_atoms)\n",
    "\n",
    "    # Check balance\n",
    "    missing_in_products = reactant_atoms - product_atoms\n",
    "    extra_in_products = product_atoms - reactant_atoms\n",
    "\n",
    "    # Remove zero counts\n",
    "    missing_in_products = +missing_in_products  # This removes zero and negative counts\n",
    "    extra_in_products = +extra_in_products\n",
    "\n",
    "    is_balanced = len(missing_in_products) == 0 and len(extra_in_products) == 0\n",
    "\n",
    "    # Calculate balance score\n",
    "    total_reactant_atoms = sum(reactant_atoms.values())\n",
    "    total_imbalance = sum(missing_in_products.values()) + sum(extra_in_products.values())\n",
    "\n",
    "    if total_reactant_atoms == 0:\n",
    "        balance_score = 0.0\n",
    "    else:\n",
    "        balance_score = max(0.0, 1.0 - (total_imbalance / total_reactant_atoms))\n",
    "\n",
    "    return {\n",
    "        'is_balanced': is_balanced,\n",
    "        'reactant_atoms': reactant_atoms,\n",
    "        'product_atoms': product_atoms,\n",
    "        'missing_in_products': missing_in_products,\n",
    "        'extra_in_products': extra_in_products,\n",
    "        'balance_score': balance_score\n",
    "    }\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def canonicalize_smiles_rdkit(smiles: str, sanitize=True) -> str:\n",
    "    if not RDKIT_AVAILABLE or not smiles or not isinstance(smiles, str):\n",
    "        return str(smiles).strip() if isinstance(smiles, str) else \"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=sanitize)\n",
    "        if mol: return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return smiles.strip()\n",
    "    except Exception:\n",
    "        if sanitize:\n",
    "            try:\n",
    "                mol_no_sanitize = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "                if mol_no_sanitize:\n",
    "                    Chem.SanitizeMol(mol_no_sanitize, Chem.SanitizeFlags.SANITIZE_FINDRADICALS | Chem.SanitizeFlags.SANITIZE_SETAROMATICITY | Chem.SanitizeFlags.SANITIZE_SETHYBRIDIZATION | Chem.SanitizeFlags.SANITIZE_SYMMRINGS, catchErrors=True)\n",
    "                    return Chem.MolToSmiles(mol_no_sanitize, canonical=True)\n",
    "            except Exception: pass\n",
    "        return smiles.strip()\n",
    "\n",
    "def standardize_generated_smiles(smiles_str: str) -> str:\n",
    "    if not RDKIT_AVAILABLE: return str(smiles_str).strip() if isinstance(smiles_str, str) else \"\"\n",
    "    if not smiles_str or not isinstance(smiles_str, str): return \"\"\n",
    "    if '.' not in smiles_str: return canonicalize_smiles_rdkit(smiles_str)\n",
    "    components = smiles_str.split('.')\n",
    "    canonical_components = sorted([canonicalize_smiles_rdkit(c) for c in components if c.strip()])\n",
    "    return '.'.join(filter(None, canonical_components))\n",
    "\n",
    "def score_chemical_validity(smiles_to_score: str) -> float:\n",
    "    \"\"\"Scores the chemical validity of a SMILES string (preferably raw model output).\"\"\"\n",
    "    if not RDKIT_AVAILABLE or not smiles_to_score or not isinstance(smiles_to_score, str):\n",
    "        return 0.0\n",
    "    try:\n",
    "        # Try to parse with sanitization first for a stricter validity check\n",
    "        mol = Chem.MolFromSmiles(smiles_to_score, sanitize=True)\n",
    "        if mol: return 1.0\n",
    "        # If strict sanitization fails, try less strict parsing\n",
    "        mol_no_sanitize = Chem.MolFromSmiles(smiles_to_score, sanitize=False)\n",
    "        if mol_no_sanitize: return 0.5 # Parsable but maybe not \"fully\" valid by strict rules\n",
    "        return 0.1 # Not even parsable without sanitization\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def robust_smiles_match(raw_predicted_smiles: str, standardized_true_products_set: set) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the standardized form of raw_predicted_smiles matches any SMILES\n",
    "    in standardized_true_products_set.\n",
    "    Uses multi-component standardization for prediction and optional fingerprint fallback.\n",
    "    \"\"\"\n",
    "    if not raw_predicted_smiles or not isinstance(raw_predicted_smiles, str):\n",
    "        return False\n",
    "\n",
    "    standardized_prediction = canonicalize_smiles_rdkit(raw_predicted_smiles)\n",
    "    if not standardized_prediction: # If standardization results in empty string\n",
    "        return False\n",
    "\n",
    "    # 1. Direct match of standardized forms\n",
    "    if standardized_prediction in standardized_true_products_set:\n",
    "        return True\n",
    "\n",
    "    # 2. Fingerprint similarity (if RDKit available and direct match failed)\n",
    "    if RDKIT_AVAILABLE:\n",
    "        try:\n",
    "            pred_mol = Chem.MolFromSmiles(standardized_prediction)\n",
    "            if not pred_mol: # If standardized isn't parsable, try raw\n",
    "                pred_mol = Chem.MolFromSmiles(raw_predicted_smiles)\n",
    "\n",
    "            if not pred_mol: # If neither form is parsable\n",
    "                return False\n",
    "\n",
    "            pred_fp = GetMorganFingerprintAsBitVect(pred_mol, 2, nBits=2048)\n",
    "\n",
    "            for true_s_standardized in standardized_true_products_set:\n",
    "                true_mol = Chem.MolFromSmiles(true_s_standardized)\n",
    "                if not true_mol: continue\n",
    "\n",
    "                true_fp = GetMorganFingerprintAsBitVect(true_mol, 2, nBits=2048)\n",
    "                similarity = TanimotoSimilarity(pred_fp, true_fp)\n",
    "                if similarity >= FINGERPRINT_SIMILARITY_THRESHOLD:\n",
    "                    return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def group_true_products_for_eval_assuming_standardized_input(dataset: Dataset) -> dict:\n",
    "    reactant_to_products_map = {}\n",
    "    for example in tqdm(dataset, desc=\"Grouping pre-standardized true products\", disable=True):\n",
    "        reactant_std = str(example.get('reactant', '')).strip()\n",
    "        product_std = str(example.get('product', '')).strip()\n",
    "        if not reactant_std or not product_std: continue\n",
    "        if reactant_std not in reactant_to_products_map:\n",
    "            reactant_to_products_map[reactant_std] = set()\n",
    "        reactant_to_products_map[reactant_std].add(product_std)\n",
    "    return reactant_to_products_map\n",
    "\n",
    "# --- Main Prediction Script for Single Reactant ---\n",
    "def predict_for_single_reactant_formatted():\n",
    "    global INPUT_REACTANT_SMILES\n",
    "\n",
    "    # 1. Load Tokenizer and Model\n",
    "    print(f\"Loading tokenizer from {ADAPTER_MODEL_PATH}...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ADAPTER_MODEL_PATH, use_fast=True)\n",
    "        if tokenizer.pad_token is None: tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        if tokenizer.eos_token is None: tokenizer.add_special_tokens({'eos_token': '</s>'})\n",
    "        if tokenizer.bos_token is None: tokenizer.add_special_tokens({'bos_token': '<s>'})\n",
    "\n",
    "    except Exception as e: print(f\"Error loading tokenizer: {e}. Exiting.\"); return\n",
    "\n",
    "    print(f\"Loading base model '{BASE_MODEL_NAME}' and adapter from '{ADAPTER_MODEL_PATH}'...\")\n",
    "    try:\n",
    "        quantization_config_bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            BASE_MODEL_NAME, quantization_config=quantization_config_bnb, device_map={\"\":DEVICE}\n",
    "        )\n",
    "        base_model.resize_token_embeddings(len(tokenizer))\n",
    "        model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_PATH)\n",
    "        model.eval(); model.to(DEVICE)\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e: print(f\"Error loading model: {e}\"); traceback.print_exc(); return\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # 2. Prepare Input Reactant and True Products (if available)\n",
    "    true_products_lookup_map = {}\n",
    "    if not INPUT_REACTANT_SMILES:\n",
    "        print(f\"INPUT_REACTANT_SMILES not set. Loading from test_data_raw_pairs[{LOAD_FROM_TEST_SET_INDEX}]...\")\n",
    "        try:\n",
    "            raw_dataset = load_from_disk(DATA_PATH)\n",
    "            test_data_raw_pairs = raw_dataset['test']\n",
    "            if LOAD_FROM_TEST_SET_INDEX < len(test_data_raw_pairs):\n",
    "                INPUT_REACTANT_SMILES = str(test_data_raw_pairs[LOAD_FROM_TEST_SET_INDEX].get('reactant', '')).strip()\n",
    "                print(f\"Using reactant from test set: {INPUT_REACTANT_SMILES}\")\n",
    "                true_products_lookup_map = group_true_products_for_eval_assuming_standardized_input(test_data_raw_pairs)\n",
    "            else:\n",
    "                print(f\"Error: Index {LOAD_FROM_TEST_SET_INDEX} out of bounds.\"); return\n",
    "        except Exception as e: print(f\"Error loading test data: {e}\"); traceback.print_exc(); return\n",
    "    elif os.path.exists(DATA_PATH):\n",
    "         try:\n",
    "            raw_dataset = load_from_disk(DATA_PATH)\n",
    "            test_data_raw_pairs = raw_dataset['test']\n",
    "            true_products_lookup_map = group_true_products_for_eval_assuming_standardized_input(test_data_raw_pairs)\n",
    "            print(\"Loaded true products map for potential matching.\")\n",
    "         except Exception as e:\n",
    "            print(f\"Could not load true products map from {DATA_PATH}: {e}.\")\n",
    "\n",
    "    if not INPUT_REACTANT_SMILES: print(\"No input reactant SMILES. Exiting.\"); return\n",
    "\n",
    "    print(f\"\\n--- Predicting for Reactant: {INPUT_REACTANT_SMILES} ---\")\n",
    "    reactant_for_lookup = INPUT_REACTANT_SMILES\n",
    "    true_standardized_products_set = true_products_lookup_map.get(reactant_for_lookup, set())\n",
    "    all_true_products_str = '; '.join(sorted(list(true_standardized_products_set))) if true_standardized_products_set else \"N/A (not in test set or no products defined)\"\n",
    "\n",
    "    # 3. Generate Predictions\n",
    "    inputs = tokenizer(INPUT_REACTANT_SMILES, return_tensors=\"pt\", max_length=MAX_LENGTH_GENERATION, truncation=True)\n",
    "    input_ids = inputs.input_ids.to(DEVICE)\n",
    "    attention_mask = inputs.attention_mask.to(DEVICE)\n",
    "\n",
    "    predictions_data = []\n",
    "    original_ranking_matches = {\"top1\": False, \"top3\": False, \"top5\": False, \"top10\": False}\n",
    "    reranked_matches = {\"top1\": False, \"top3\": False, \"top5\": False, \"top10\": False}\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids, attention_mask=attention_mask,\n",
    "                max_length=MAX_LENGTH_GENERATION, num_return_sequences=NUM_RETURN_SEQUENCES,\n",
    "                num_beams=NUM_BEAMS, do_sample=DO_SAMPLE,\n",
    "                temperature=TEMPERATURE if DO_SAMPLE else 1.0,\n",
    "                top_k=TOP_K if DO_SAMPLE else None, top_p=TOP_P if DO_SAMPLE else None,\n",
    "                early_stopping=True, eos_token_id=model.config.eos_token_id,\n",
    "                pad_token_id=model.config.pad_token_id,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "                output_scores=True, return_dict_in_generate=True\n",
    "            )\n",
    "\n",
    "        raw_predicted_smiles_batch = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "\n",
    "        print(\"\\n--- Original Model Ranking ---\")\n",
    "\n",
    "        # First evaluate with original model ranking\n",
    "        original_order_predictions = []\n",
    "        seen_original = set()\n",
    "        for i, pred_smiles_raw in enumerate(raw_predicted_smiles_batch):\n",
    "            standardized_pred = standardize_generated_smiles(pred_smiles_raw)\n",
    "            if standardized_pred and standardized_pred not in seen_original:\n",
    "                original_order_predictions.append(standardized_pred)\n",
    "                seen_original.add(standardized_pred)\n",
    "                print(f\"  Rank {i+1}: {standardized_pred}\")\n",
    "\n",
    "        # Check original ranking matches\n",
    "        if true_standardized_products_set:\n",
    "            if original_order_predictions and robust_smiles_match(original_order_predictions[0], true_standardized_products_set):\n",
    "                original_ranking_matches[\"top1\"] = True\n",
    "            if any(robust_smiles_match(p, true_standardized_products_set) for p in original_order_predictions[:3]):\n",
    "                original_ranking_matches[\"top3\"] = True\n",
    "            if any(robust_smiles_match(p, true_standardized_products_set) for p in original_order_predictions[:5]):\n",
    "                original_ranking_matches[\"top5\"] = True\n",
    "            if any(robust_smiles_match(p, true_standardized_products_set) for p in original_order_predictions[:10]):\n",
    "                original_ranking_matches[\"top10\"] = True\n",
    "\n",
    "        # Now calculate comprehensive scores\n",
    "        for i, pred_smiles_raw in enumerate(raw_predicted_smiles_batch):\n",
    "            pred_smiles_standardized = standardize_generated_smiles(pred_smiles_raw)\n",
    "\n",
    "            # Model score: higher rank (lower index) gets higher score\n",
    "            model_score = 1.0 - (i / NUM_RETURN_SEQUENCES)\n",
    "\n",
    "            # Validity score\n",
    "            validity_score = score_chemical_validity(pred_smiles_raw)\n",
    "\n",
    "            # Balance score\n",
    "            balance_score = 0.0\n",
    "            if pred_smiles_standardized:\n",
    "                balance_result = check_atom_balance(INPUT_REACTANT_SMILES, pred_smiles_standardized)\n",
    "                balance_score = balance_result['balance_score']\n",
    "\n",
    "            # Comprehensive score\n",
    "            comprehensive_score = (model_score * MODEL_SCORE_WEIGHT +\n",
    "                                 validity_score * VALIDITY_SCORE_WEIGHT +\n",
    "                                 balance_score * BALANCE_SCORE_WEIGHT)\n",
    "\n",
    "            matches_ground_truth = False\n",
    "            matching_true_product_for_row = \"N/A\"\n",
    "            max_tanimoto_to_true = 0.0\n",
    "\n",
    "            if true_standardized_products_set:\n",
    "                if pred_smiles_standardized in true_standardized_products_set:\n",
    "                    matches_ground_truth = True\n",
    "                    matching_true_product_for_row = pred_smiles_standardized\n",
    "\n",
    "                if RDKIT_AVAILABLE and pred_smiles_standardized:\n",
    "                    try:\n",
    "                        pred_mol = Chem.MolFromSmiles(pred_smiles_standardized)\n",
    "                        if pred_mol:\n",
    "                            pred_fp = GetMorganFingerprintAsBitVect(pred_mol, 2, nBits=2048)\n",
    "                            for true_std_smi in true_standardized_products_set:\n",
    "                                true_mol = Chem.MolFromSmiles(true_std_smi)\n",
    "                                if true_mol:\n",
    "                                    true_fp = GetMorganFingerprintAsBitVect(true_mol, 2, nBits=2048)\n",
    "                                    similarity = TanimotoSimilarity(pred_fp, true_fp)\n",
    "                                    max_tanimoto_to_true = max(max_tanimoto_to_true, similarity)\n",
    "                    except Exception:\n",
    "                        max_tanimoto_to_true = -1.0\n",
    "\n",
    "            predictions_data.append({\n",
    "                \"reactant\": INPUT_REACTANT_SMILES,\n",
    "                \"predicted_product_raw\": pred_smiles_raw,\n",
    "                \"predicted_product\": pred_smiles_standardized,\n",
    "                \"model_score\": model_score,\n",
    "                \"validity_score\": validity_score,\n",
    "                \"balance_score\": balance_score,\n",
    "                \"comprehensive_score\": comprehensive_score,\n",
    "                \"max_tanimoto_to_true\": max_tanimoto_to_true if max_tanimoto_to_true >= 0 else \"N/A\",\n",
    "                \"original_rank\": i + 1,\n",
    "                \"matches_ground_truth\": matches_ground_truth,\n",
    "                \"matching_true_product\": matching_true_product_for_row,\n",
    "                \"all_true_products_for_reactant\": all_true_products_str\n",
    "            })\n",
    "\n",
    "        # Sort by comprehensive score\n",
    "        predictions_data.sort(key=lambda x: x[\"comprehensive_score\"], reverse=True)\n",
    "\n",
    "        print(\"\\n--- Comprehensive Score Reranking ---\")\n",
    "\n",
    "        # Evaluate reranked results\n",
    "        reranked_predictions = []\n",
    "        seen_reranked = set()\n",
    "        for rerank_idx, item in enumerate(predictions_data):\n",
    "            item[\"reranked_position\"] = rerank_idx + 1\n",
    "\n",
    "            if item['predicted_product'] and item['predicted_product'] not in seen_reranked:\n",
    "                reranked_predictions.append(item['predicted_product'])\n",
    "                seen_reranked.add(item['predicted_product'])\n",
    "\n",
    "            print(f\"  Reranked {item['reranked_position']} (Orig Rank {item['original_rank']}):\")\n",
    "            print(f\"    Prediction: {item['predicted_product']}\")\n",
    "            print(f\"    Model Score: {item['model_score']:.3f}, Validity: {item['validity_score']:.3f}, Balance: {item['balance_score']:.3f}\")\n",
    "            print(f\"    Comprehensive Score: {item['comprehensive_score']:.4f}\")\n",
    "            print(f\"    Matches GT: {item['matches_ground_truth']}\")\n",
    "            if item['max_tanimoto_to_true'] != \"N/A\":\n",
    "                print(f\"    Max Tanimoto: {item['max_tanimoto_to_true']:.3f}\")\n",
    "\n",
    "        # Check reranked matches\n",
    "        if true_standardized_products_set:\n",
    "            if reranked_predictions and robust_smiles_match(reranked_predictions[0], true_standardized_products_set):\n",
    "                reranked_matches[\"top1\"] = True\n",
    "            if any(robust_smiles_match(p, true_standardized_products_set) for p in reranked_predictions[:3]):\n",
    "                reranked_matches[\"top3\"] = True\n",
    "            if any(robust_smiles_match(p, true_standardized_products_set) for p in reranked_predictions[:5]):\n",
    "                reranked_matches[\"top5\"] = True\n",
    "            if any(robust_smiles_match(p, true_standardized_products_set) for p in reranked_predictions[:10]):\n",
    "                reranked_matches[\"top10\"] = True\n",
    "\n",
    "    except Exception as e_gen:\n",
    "        print(f\"Error during generation: {e_gen}\"); traceback.print_exc()\n",
    "        # Fill with error rows\n",
    "        for rank_idx in range(NUM_RETURN_SEQUENCES):\n",
    "             predictions_data.append({\n",
    "                \"reactant\": INPUT_REACTANT_SMILES,\n",
    "                \"predicted_product_raw\": \"ERROR\",\n",
    "                \"predicted_product\": \"ERROR\",\n",
    "                \"model_score\": 0.0,\n",
    "                \"validity_score\": 0.0,\n",
    "                \"balance_score\": 0.0,\n",
    "                \"comprehensive_score\": 0.0,\n",
    "                \"max_tanimoto_to_true\": \"N/A\",\n",
    "                \"original_rank\": rank_idx + 1,\n",
    "                \"reranked_position\": rank_idx + 1,\n",
    "                \"matches_ground_truth\": False,\n",
    "                \"matching_true_product\": \"N/A\",\n",
    "                \"all_true_products_for_reactant\": all_true_products_str\n",
    "            })\n",
    "\n",
    "    # Print comparison results\n",
    "    if true_standardized_products_set:\n",
    "        print(f\"\\n--- Accuracy Comparison ---\")\n",
    "        print(f\"True Products: {all_true_products_str}\")\n",
    "        print(f\"\\nOriginal Model Ranking:\")\n",
    "        print(f\"  Top-1: {'✓' if original_ranking_matches['top1'] else '✗'}\")\n",
    "        print(f\"  Top-3: {'✓' if original_ranking_matches['top3'] else '✗'}\")\n",
    "        print(f\"  Top-5: {'✓' if original_ranking_matches['top5'] else '✗'}\")\n",
    "        print(f\"  Top-10: {'✓' if original_ranking_matches['top10'] else '✗'}\")\n",
    "\n",
    "        print(f\"\\nComprehensive Score Reranking:\")\n",
    "        print(f\"  Top-1: {'✓' if reranked_matches['top1'] else '✗'}\")\n",
    "        print(f\"  Top-3: {'✓' if reranked_matches['top3'] else '✗'}\")\n",
    "        print(f\"  Top-5: {'✓' if reranked_matches['top5'] else '✗'}\")\n",
    "        print(f\"  Top-10: {'✓' if reranked_matches['top10'] else '✗'}\")\n",
    "\n",
    "        print(f\"\\nImprovement:\")\n",
    "        for k in [\"top1\", \"top3\", \"top5\", \"top10\"]:\n",
    "            improvement = \"+\" if reranked_matches[k] and not original_ranking_matches[k] else \"-\" if original_ranking_matches[k] and not reranked_matches[k] else \"=\"\n",
    "            print(f\"  {k.upper()}: {improvement}\")\n",
    "    else:\n",
    "        print(f\"\\n--- No Ground Truth Available for Comparison ---\")\n",
    "\n",
    "    # 5. Save to CSV\n",
    "    if predictions_data:\n",
    "        column_order = [\n",
    "            \"reactant\", \"predicted_product\",\n",
    "            \"model_score\", \"validity_score\", \"balance_score\", \"comprehensive_score\",\n",
    "            \"max_tanimoto_to_true\", \"original_rank\", \"reranked_position\",\n",
    "            \"matches_ground_truth\", \"predicted_product_raw\", \"matching_true_product\",\n",
    "            \"all_true_products_for_reactant\"\n",
    "        ]\n",
    "        df_results = pd.DataFrame(predictions_data)\n",
    "\n",
    "        # Ensure all desired columns are present\n",
    "        for col in column_order:\n",
    "            if col not in df_results.columns:\n",
    "                df_results[col] = \"N/A\"\n",
    "\n",
    "        df_results = df_results[column_order]\n",
    "\n",
    "        try:\n",
    "            df_results.to_csv(OUTPUT_CSV_PATH_SINGLE_FORMATTED, index=False, float_format='%.4f')\n",
    "            print(f\"\\nDetailed predictions saved to {OUTPUT_CSV_PATH_SINGLE_FORMATTED}\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"Error saving to CSV: {e_csv}\")\n",
    "    else:\n",
    "        print(\"No results generated.\")\n",
    "\n",
    "    del model, base_model, tokenizer, inputs, input_ids, attention_mask\n",
    "    if 'outputs' in locals(): del outputs\n",
    "    gc.collect()\n",
    "    if DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_for_single_reactant_formatted()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
