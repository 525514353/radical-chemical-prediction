{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T16:58:27.813800Z",
     "start_time": "2025-06-06T16:57:22.423687Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Dataset Split: test\n",
      "  Reactant SMILES: C=CC(C)C.[OH]\n",
      "  Device: cuda\n",
      "Initializing Chemical Reaction Explainer...\n",
      "Loading data from './data/data' using test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping data: 100%|██████████| 508/508 [00:00<00:00, 28178.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 508 samples from test split, 429 unique reactants\n",
      "Found reactant in dataset with 2 product(s) and reaction type: addition\n",
      "\n",
      "Starting analysis for reactant: C=CC(C)C.[OH]\n",
      "\n",
      "============================================================\n",
      "Analyzing reactant: C=CC(C)C.[OH]\n",
      "Reaction type: addition\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target: [CH2]C(O)C(C)C\n",
      "Computing attributions...\n",
      "\n",
      "--- GLOBAL ATTRIBUTION SCORES (INCLUDING DOTS) ---\n",
      "Input tokens and their global attribution scores:\n",
      "  Token[0]: 'C=CC' -> Global Attribution: 0.627311\n",
      "  Token[1]: '(C)C' -> Global Attribution: 0.297719\n",
      "  Token[2]: '.' -> Global Attribution: 0.000000\n",
      "  Token[3]: '[OH]' -> Global Attribution: 0.122509\n",
      "========================================\n",
      "\n",
      "\n",
      "--- GLOBAL ATTRIBUTION SCORES (INCLUDING DOTS) ---\n",
      "Input tokens and their global attribution scores:\n",
      "  Token[0]: 'C=CC' -> Global Attribution: 0.627311\n",
      "  Token[1]: '(C)C' -> Global Attribution: 0.297719\n",
      "  Token[2]: '.' -> Global Attribution: 0.000000\n",
      "  Token[3]: '[OH]' -> Global Attribution: 0.122509\n",
      "========================================\n",
      "\n",
      "Creating comparison plot for reactant: C=CC(C)C.[OH]...\n",
      "\n",
      "==================================================\n",
      "TOKEN TO ATOM MAPPING\n",
      "==================================================\n",
      "SMILES: C=CC(C)C.[OH]\n",
      "Number of atoms in molecule: 6\n",
      "Atom symbols: ['C', 'C', 'C', 'C', 'C', 'O']\n",
      "\n",
      "Filtered tokens and their attribution scores:\n",
      "  Token[0]: 'C=CC' -> Attribution: 0.627311\n",
      "  Token[1]: '(C)C' -> Attribution: 0.297719\n",
      "  Token[2]: '[OH]' -> Attribution: 0.122509\n",
      "\n",
      "--- ROUND 1: Substructure Matching ---\n",
      "Token 'C=CC' (attr: 0.627311) -> Found 1 potential matches: ((0, 1, 2),)\n",
      "  ✓ MATCHED: Token 'C=CC' -> Atoms (0, 1, 2) (symbols: ['C', 'C', 'C']) with attribution 0.627311\n",
      "Token '(C)C' (attr: 0.297719) -> Found 3 potential matches: ((1, 2), (2, 3), (2, 4))\n",
      "  ✗ No unassigned atoms available for token '(C)C'\n",
      "Token '[OH]' (attr: 0.122509) -> Found 1 potential matches: ((5,),)\n",
      "  ✓ MATCHED: Token '[OH]' -> Atoms (5,) (symbols: ['O']) with attribution 0.122509\n",
      "\n",
      "--- ROUND 2: Sequential Assignment ---\n",
      "Unmatched tokens: ['(C)C']\n",
      "  ✓ SEQUENTIAL: Token '(C)C' -> Atoms [3, 4] (symbols: ['C', 'C']) with attribution 0.297719\n",
      "\n",
      "--- FINAL ATOM CONTRIBUTIONS ---\n",
      "Total assigned atoms: 6 / 6\n",
      "Atom contributions:\n",
      "  Atom[0] (C): 0.627311 ✓\n",
      "  Atom[1] (C): 0.627311 ✓\n",
      "  Atom[2] (C): 0.627311 ✓\n",
      "  Atom[3] (C): 0.297719 ✓\n",
      "  Atom[4] (C): 0.297719 ✓\n",
      "  Atom[5] (O): 0.122509 ✓\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- ATOM AND BOND HIGHLIGHTING DETAILS ---\n",
      "Maximum contribution value: 0.627311\n",
      "Atom highlighting details:\n",
      "  Atom[0] (C): contribution=0.627311, normalized=1.000, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Atom[1] (C): contribution=0.627311, normalized=1.000, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Atom[2] (C): contribution=0.627311, normalized=1.000, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Atom[3] (C): contribution=0.297719, normalized=0.475, intensity=0.580, RGB=(0.463, 0.532, 0.590)\n",
      "  Atom[4] (C): contribution=0.297719, normalized=0.475, intensity=0.580, RGB=(0.463, 0.532, 0.590)\n",
      "  Atom[5] (O): contribution=0.122509, normalized=0.195, intensity=0.356, RGB=(0.648, 0.705, 0.722)\n",
      "\n",
      "Bond highlighting details:\n",
      "  Bond[0] (C0-C1): atom1_contrib=0.627311, atom2_contrib=0.627311, avg_contrib=0.627311, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Bond[1] (C1-C2): atom1_contrib=0.627311, atom2_contrib=0.627311, avg_contrib=0.627311, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Bond[2] (C2-C3): atom1_contrib=0.627311, atom2_contrib=0.297719, avg_contrib=0.462515, intensity=0.790, RGB=(0.287, 0.368, 0.464)\n",
      "  Bond[3] (C2-C4): atom1_contrib=0.627311, atom2_contrib=0.297719, avg_contrib=0.462515, intensity=0.790, RGB=(0.287, 0.368, 0.464)\n",
      "========================================\n",
      "\n",
      "Creating comparison plot for reactant: C=CC(C)C.[OH]...\n",
      "\n",
      "==================================================\n",
      "TOKEN TO ATOM MAPPING\n",
      "==================================================\n",
      "SMILES: C=CC(C)C.[OH]\n",
      "Number of atoms in molecule: 6\n",
      "Atom symbols: ['C', 'C', 'C', 'C', 'C', 'O']\n",
      "\n",
      "Filtered tokens and their attribution scores:\n",
      "  Token[0]: 'C=CC' -> Attribution: 0.627311\n",
      "  Token[1]: '(C)C' -> Attribution: 0.297719\n",
      "  Token[2]: '[OH]' -> Attribution: 0.122509\n",
      "\n",
      "--- ROUND 1: Substructure Matching ---\n",
      "Token 'C=CC' (attr: 0.627311) -> Found 1 potential matches: ((0, 1, 2),)\n",
      "  ✓ MATCHED: Token 'C=CC' -> Atoms (0, 1, 2) (symbols: ['C', 'C', 'C']) with attribution 0.627311\n",
      "Token '(C)C' (attr: 0.297719) -> Found 3 potential matches: ((1, 2), (2, 3), (2, 4))\n",
      "  ✗ No unassigned atoms available for token '(C)C'\n",
      "Token '[OH]' (attr: 0.122509) -> Found 1 potential matches: ((5,),)\n",
      "  ✓ MATCHED: Token '[OH]' -> Atoms (5,) (symbols: ['O']) with attribution 0.122509\n",
      "\n",
      "--- ROUND 2: Sequential Assignment ---\n",
      "Unmatched tokens: ['(C)C']\n",
      "  ✓ SEQUENTIAL: Token '(C)C' -> Atoms [3, 4] (symbols: ['C', 'C']) with attribution 0.297719\n",
      "\n",
      "--- FINAL ATOM CONTRIBUTIONS ---\n",
      "Total assigned atoms: 6 / 6\n",
      "Atom contributions:\n",
      "  Atom[0] (C): 0.627311 ✓\n",
      "  Atom[1] (C): 0.627311 ✓\n",
      "  Atom[2] (C): 0.627311 ✓\n",
      "  Atom[3] (C): 0.297719 ✓\n",
      "  Atom[4] (C): 0.297719 ✓\n",
      "  Atom[5] (O): 0.122509 ✓\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- ATOM AND BOND HIGHLIGHTING DETAILS ---\n",
      "Maximum contribution value: 0.627311\n",
      "Atom highlighting details:\n",
      "  Atom[0] (C): contribution=0.627311, normalized=1.000, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Atom[1] (C): contribution=0.627311, normalized=1.000, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Atom[2] (C): contribution=0.627311, normalized=1.000, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Atom[3] (C): contribution=0.297719, normalized=0.475, intensity=0.580, RGB=(0.463, 0.532, 0.590)\n",
      "  Atom[4] (C): contribution=0.297719, normalized=0.475, intensity=0.580, RGB=(0.463, 0.532, 0.590)\n",
      "  Atom[5] (O): contribution=0.122509, normalized=0.195, intensity=0.356, RGB=(0.648, 0.705, 0.722)\n",
      "\n",
      "Bond highlighting details:\n",
      "  Bond[0] (C0-C1): atom1_contrib=0.627311, atom2_contrib=0.627311, avg_contrib=0.627311, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Bond[1] (C1-C2): atom1_contrib=0.627311, atom2_contrib=0.627311, avg_contrib=0.627311, intensity=1.000, RGB=(0.114, 0.208, 0.341)\n",
      "  Bond[2] (C2-C3): atom1_contrib=0.627311, atom2_contrib=0.297719, avg_contrib=0.462515, intensity=0.790, RGB=(0.287, 0.368, 0.464)\n",
      "  Bond[3] (C2-C4): atom1_contrib=0.627311, atom2_contrib=0.297719, avg_contrib=0.462515, intensity=0.790, RGB=(0.287, 0.368, 0.464)\n",
      "========================================\n",
      "\n",
      "Saved: ./explainability_results\\reactant_C=CC(C)C.[OH]\\reaction_comparison.png\n",
      "✓ Analysis completed successfully\n",
      "\n",
      "============================================================\n",
      "Analysis Summary:\n",
      "Reactant: C=CC(C)C.[OH]\n",
      "Reaction Type: addition\n",
      "True Products: 2\n",
      "Valid Predictions: 3\n",
      "Results saved in: ./explainability_results\n",
      "Analysis completed successfully!\n",
      "Saved: ./explainability_results\\reactant_C=CC(C)C.[OH]\\reaction_comparison.png\n",
      "✓ Analysis completed successfully\n",
      "\n",
      "============================================================\n",
      "Analysis Summary:\n",
      "Reactant: C=CC(C)C.[OH]\n",
      "Reaction Type: addition\n",
      "True Products: 2\n",
      "Valid Predictions: 3\n",
      "Results saved in: ./explainability_results\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from captum.attr import IntegratedGradients\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Set, Optional\n",
    "import warnings\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# 检查版本兼容性\n",
    "try:\n",
    "    from transformers.cache_utils import EncoderDecoderCache\n",
    "    TRANSFORMERS_NEW_CACHE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_NEW_CACHE = False\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    RDKIT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RDKIT_AVAILABLE = False\n",
    "    print(\"Warning: RDKit not available.\")\n",
    "\n",
    "# 统一配色方案\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB', 'light_blue': '#A8DADC', 'dark_blue': '#1D3557',\n",
    "    'background': '#F1FAEE', 'success': '#00B04F', 'error': '#EB5757',\n",
    "    'correct_highlight': '#FF6B35', 'neutral': '#828282'\n",
    "}\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Times New Roman', 'font.size': 16, 'axes.linewidth': 1.5,\n",
    "    'figure.dpi': 600, 'savefig.dpi': 600, 'axes.grid': False,\n",
    "    'axes.titlesize': 18, 'axes.labelsize': 16\n",
    "})\n",
    "\n",
    "def canonicalize_smiles_rdkit(smiles: str, sanitize=True) -> str:\n",
    "    if not RDKIT_AVAILABLE or not smiles:\n",
    "        return str(smiles).strip() if smiles else \"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=sanitize)\n",
    "        if mol:\n",
    "            result = Chem.MolToSmiles(mol, canonical=True)\n",
    "            del mol\n",
    "            return result\n",
    "        return smiles.strip()\n",
    "    except Exception:\n",
    "        return smiles.strip()\n",
    "\n",
    "def group_data_mappings(dataset) -> Tuple[dict, dict]:\n",
    "    \"\"\"同时创建产品和反应类型映射\"\"\"\n",
    "    products_map = {}\n",
    "    types_map = {}\n",
    "    for example in tqdm(dataset, desc=\"Grouping data\"):\n",
    "        reactant = str(example.get('reactant', '')).strip()\n",
    "        if reactant:\n",
    "            product = str(example.get('product', '')).strip()\n",
    "            if product:\n",
    "                if reactant not in products_map:\n",
    "                    products_map[reactant] = set()\n",
    "                products_map[reactant].add(product)\n",
    "            types_map[reactant] = str(example.get('type', 'Unknown')).strip()\n",
    "    return products_map, types_map\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"统一的内存清理函数\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def get_font(size: int):\n",
    "    \"\"\"简化的字体获取\"\"\"\n",
    "    paths = [\"/System/Library/Fonts/Times.ttc\", \"C:/Windows/Fonts/times.ttf\",\n",
    "             \"/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf\"]\n",
    "    for path in paths:\n",
    "        try:\n",
    "            return ImageFont.truetype(path, size)\n",
    "        except:\n",
    "            continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "def format_smiles_multiline(smiles: str, max_chars: int = 70) -> str:\n",
    "    \"\"\"格式化SMILES为多行\"\"\"\n",
    "    if len(smiles) <= max_chars:\n",
    "        return smiles\n",
    "    first = smiles[:max_chars]\n",
    "    second = smiles[max_chars:]\n",
    "    if len(second) > max_chars:\n",
    "        second = second[:max_chars-3] + \"...\"\n",
    "    return f\"{first}\\n{second}\"\n",
    "\n",
    "class ChemicalReactionExplainer:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "\n",
    "        # 设置模型配置\n",
    "        for attr in ['bos_token_id', 'eos_token_id', 'pad_token_id']:\n",
    "            setattr(self.model.config, attr, getattr(tokenizer, attr))\n",
    "        self.model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        self.output_dir = \"./explainability_results\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        # 预计算特殊token（移除\".\"，因为它需要在global attribution中显示）\n",
    "        self.special_tokens = {\n",
    "            tokenizer.pad_token, tokenizer.eos_token, tokenizer.bos_token,\n",
    "            tokenizer.unk_token, '<pad>', '</s>', '<s>', '<unk>'\n",
    "        }\n",
    "\n",
    "    def sanitize_smiles(self, smiles: str) -> Tuple[Optional[str], bool]:\n",
    "        if not smiles or smiles.strip() == \"\":\n",
    "            return None, False\n",
    "        try:\n",
    "            canonical = canonicalize_smiles_rdkit(smiles.strip())\n",
    "            if canonical:\n",
    "                mol = Chem.MolFromSmiles(canonical)\n",
    "                if mol is not None:\n",
    "                    del mol\n",
    "                    return canonical, True\n",
    "            return None, False\n",
    "        except Exception:\n",
    "            return None, False\n",
    "\n",
    "    def beam_search_predict(self, reactant_smiles: str, num_beams: int = 3, max_length: int = 256) -> List[Dict]:\n",
    "        inputs = self.tokenizer(reactant_smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        generate_kwargs = {\n",
    "            'num_beams': 30, 'num_return_sequences': num_beams, 'max_length': max_length,\n",
    "            'do_sample': False, 'return_dict_in_generate': True, 'output_scores': True,\n",
    "            'early_stopping': True, 'use_cache': False,\n",
    "            'eos_token_id': self.model.config.eos_token_id,\n",
    "            'pad_token_id': self.model.config.pad_token_id,\n",
    "            'decoder_start_token_id': self.model.config.decoder_start_token_id\n",
    "        }\n",
    "\n",
    "        if not TRANSFORMERS_NEW_CACHE:\n",
    "            generate_kwargs['past_key_values'] = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **generate_kwargs)\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(min(num_beams, len(outputs.sequences))):\n",
    "            pred_text = self.tokenizer.decode(outputs.sequences[i], skip_special_tokens=True)\n",
    "            sanitized, is_valid = self.sanitize_smiles(pred_text)\n",
    "            predictions.append({\n",
    "                'raw_smiles': pred_text, 'sanitized_smiles': sanitized,\n",
    "                'is_valid': is_valid, 'rank': i + 1\n",
    "            })\n",
    "\n",
    "        del outputs, inputs\n",
    "        clear_memory()\n",
    "        return predictions\n",
    "\n",
    "    def filter_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"过滤特殊token，保留\".\"用于global attribution显示\"\"\"\n",
    "        return [token for token in tokens if token not in self.special_tokens and token is not None]\n",
    "\n",
    "    def parse_token_to_mol(self, token: str):\n",
    "        \"\"\"解析token为分子对象\"\"\"\n",
    "        if not token or token in self.special_tokens or token == '.':\n",
    "            return None\n",
    "\n",
    "        # 尝试多种解析方法\n",
    "        parse_attempts = [\n",
    "            token,  # 直接解析\n",
    "            re.sub(r'[0-9()=#+\\-@/\\\\]', '', token),  # 移除特殊字符\n",
    "            f'[{token}]' if len(token) <= 3 and token.isalpha() else None  # 单原子\n",
    "        ]\n",
    "\n",
    "        for attempt in parse_attempts:\n",
    "            if attempt:\n",
    "                try:\n",
    "                    mol = Chem.MolFromSmiles(attempt)\n",
    "                    if mol and mol.GetNumAtoms() > 0:\n",
    "                        return mol\n",
    "                except:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    def map_tokens_to_atoms(self, smiles: str, tokens: List[str], attributions: np.ndarray) -> Dict[int, float]:\n",
    "        \"\"\"映射token到原子\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return {}\n",
    "\n",
    "        atom_contributions = {}\n",
    "        assigned_atoms = set()\n",
    "\n",
    "        # 过滤并配对token和attribution，但排除\".\"token\n",
    "        filtered_items = [(token, attr) for token, attr in zip(tokens, attributions)\n",
    "                         if token not in self.special_tokens and token != '.']\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TOKEN TO ATOM MAPPING\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"SMILES: {smiles}\")\n",
    "        print(f\"Number of atoms in molecule: {mol.GetNumAtoms()}\")\n",
    "        print(f\"Atom symbols: {[mol.GetAtoms()[i].GetSymbol() for i in range(mol.GetNumAtoms())]}\")\n",
    "        print(f\"\\nFiltered tokens and their attribution scores:\")\n",
    "        for i, (token, attr) in enumerate(filtered_items):\n",
    "            print(f\"  Token[{i}]: '{token}' -> Attribution: {attr:.6f}\")\n",
    "\n",
    "        # 第一轮：子结构匹配\n",
    "        print(f\"\\n--- ROUND 1: Substructure Matching ---\")\n",
    "        successful_matches = set()\n",
    "        for i, (token, attribution) in enumerate(filtered_items):\n",
    "            token_mol = self.parse_token_to_mol(token)\n",
    "            if token_mol is not None:\n",
    "                try:\n",
    "                    matches = mol.GetSubstructMatches(token_mol)\n",
    "                    print(f\"Token '{token}' (attr: {attribution:.6f}) -> Found {len(matches)} potential matches: {matches}\")\n",
    "\n",
    "                    for match in matches:\n",
    "                        if all(atom_idx not in assigned_atoms for atom_idx in match):\n",
    "                            for atom_idx in match:\n",
    "                                atom_contributions[atom_idx] = attribution\n",
    "                                assigned_atoms.add(atom_idx)\n",
    "                            successful_matches.add(i)\n",
    "                            atom_symbols = [mol.GetAtoms()[idx].GetSymbol() for idx in match]\n",
    "                            print(f\"  ✓ MATCHED: Token '{token}' -> Atoms {match} (symbols: {atom_symbols}) with attribution {attribution:.6f}\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"  ✗ No unassigned atoms available for token '{token}'\")\n",
    "                    del token_mol\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error matching '{token}': {e}\")\n",
    "                    if token_mol:\n",
    "                        del token_mol\n",
    "            else:\n",
    "                print(f\"Token '{token}' could not be parsed to molecule\")\n",
    "\n",
    "        # 第二轮：顺序分配未匹配的token\n",
    "        unmatched_items = [filtered_items[i] for i in range(len(filtered_items)) if i not in successful_matches]\n",
    "\n",
    "        if unmatched_items:\n",
    "            print(f\"\\n--- ROUND 2: Sequential Assignment ---\")\n",
    "            print(f\"Unmatched tokens: {[item[0] for item in unmatched_items]}\")\n",
    "\n",
    "            next_atom = 0\n",
    "            for token, attribution in unmatched_items:\n",
    "                # 跳到下一个未分配的原子\n",
    "                while next_atom in assigned_atoms and next_atom < mol.GetNumAtoms():\n",
    "                    next_atom += 1\n",
    "\n",
    "                token_mol = self.parse_token_to_mol(token)\n",
    "                atom_count = token_mol.GetNumAtoms() if token_mol else 1\n",
    "\n",
    "                assigned_for_token = []\n",
    "                for _ in range(atom_count):\n",
    "                    if next_atom < mol.GetNumAtoms():\n",
    "                        atom_contributions[next_atom] = attribution\n",
    "                        assigned_atoms.add(next_atom)\n",
    "                        assigned_for_token.append(next_atom)\n",
    "                        next_atom += 1\n",
    "                        while next_atom in assigned_atoms and next_atom < mol.GetNumAtoms():\n",
    "                            next_atom += 1\n",
    "\n",
    "                if assigned_for_token:\n",
    "                    atom_symbols = [mol.GetAtoms()[idx].GetSymbol() for idx in assigned_for_token]\n",
    "                    print(f\"  ✓ SEQUENTIAL: Token '{token}' -> Atoms {assigned_for_token} (symbols: {atom_symbols}) with attribution {attribution:.6f}\")\n",
    "                else:\n",
    "                    print(f\"  ✗ No atoms available for token '{token}'\")\n",
    "\n",
    "                if token_mol:\n",
    "                    del token_mol\n",
    "\n",
    "        # 打印最终的原子贡献\n",
    "        print(f\"\\n--- FINAL ATOM CONTRIBUTIONS ---\")\n",
    "        print(f\"Total assigned atoms: {len(assigned_atoms)} / {mol.GetNumAtoms()}\")\n",
    "\n",
    "        if atom_contributions:\n",
    "            print(\"Atom contributions:\")\n",
    "            for atom_idx in range(mol.GetNumAtoms()):\n",
    "                contribution = atom_contributions.get(atom_idx, 0.0)\n",
    "                atom_symbol = mol.GetAtoms()[atom_idx].GetSymbol()\n",
    "                status = \"✓\" if atom_idx in assigned_atoms else \"✗\"\n",
    "                print(f\"  Atom[{atom_idx}] ({atom_symbol}): {contribution:.6f} {status}\")\n",
    "        else:\n",
    "            print(\"No atom contributions found!\")\n",
    "\n",
    "        unassigned = set(range(mol.GetNumAtoms())) - assigned_atoms\n",
    "        if unassigned:\n",
    "            unassigned_symbols = [mol.GetAtoms()[idx].GetSymbol() for idx in unassigned]\n",
    "            print(f\"\\n⚠️  UNASSIGNED ATOMS: {sorted(unassigned)} (symbols: {unassigned_symbols})\")\n",
    "\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "        del mol\n",
    "        return atom_contributions\n",
    "\n",
    "    def compute_attribution_scores(self, input_text: str, target_text: str) -> Tuple[np.ndarray, List[str], List[str]]:\n",
    "        \"\"\"计算归因分数\"\"\"\n",
    "        print(\"Computing attributions...\")\n",
    "\n",
    "        # 编码输入\n",
    "        input_encoding = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        target_encoding = self.tokenizer(target_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "\n",
    "        input_ids = input_encoding['input_ids'].to(self.device)\n",
    "        target_ids = target_encoding['input_ids'].to(self.device)\n",
    "\n",
    "        input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        target_tokens = self.tokenizer.convert_ids_to_tokens(target_ids[0])\n",
    "\n",
    "        # 准备decoder输入\n",
    "        decoder_input_ids = target_ids.clone()\n",
    "        if decoder_input_ids[0, 0] != self.tokenizer.bos_token_id:\n",
    "            decoder_input_ids = torch.cat([\n",
    "                torch.tensor([[self.tokenizer.bos_token_id]], device=self.device), decoder_input_ids\n",
    "            ], dim=1)\n",
    "\n",
    "        # 获取embeddings\n",
    "        input_embeds = self.model.get_encoder().embed_tokens(input_ids)\n",
    "        baseline_embeds = self.model.get_encoder().embed_tokens(\n",
    "            torch.full_like(input_ids, self.tokenizer.pad_token_id)\n",
    "        )\n",
    "\n",
    "        max_target_len = min(len(target_tokens), decoder_input_ids.shape[1] - 1)\n",
    "        max_input_len = min(len(input_tokens), input_embeds.shape[1])\n",
    "\n",
    "        # 计算归因矩阵\n",
    "        attribution_matrix = []\n",
    "        for target_pos in range(max_target_len):\n",
    "            def forward_func(input_embeds_var):\n",
    "                decoder_embeds = self.model.get_decoder().embed_tokens(decoder_input_ids)\n",
    "                outputs = self.model(\n",
    "                    inputs_embeds=input_embeds_var,\n",
    "                    decoder_inputs_embeds=decoder_embeds,\n",
    "                    return_dict=True, use_cache=False\n",
    "                )\n",
    "                if target_pos < outputs.logits.shape[1]:\n",
    "                    return outputs.logits[:, target_pos, :].max(dim=-1)[0]\n",
    "                return torch.zeros(outputs.logits.shape[0], device=outputs.logits.device)\n",
    "\n",
    "            try:\n",
    "                ig = IntegratedGradients(forward_func)\n",
    "                attributions = ig.attribute(\n",
    "                    inputs=input_embeds, baselines=baseline_embeds,\n",
    "                    n_steps=50, internal_batch_size=1\n",
    "                )\n",
    "                token_attributions = attributions.sum(dim=-1).squeeze(0)[:max_input_len]\n",
    "                attribution_matrix.append(token_attributions.detach().cpu().numpy())\n",
    "                del attributions, token_attributions\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed at position {target_pos}: {e}\")\n",
    "                attribution_matrix.append(np.zeros(max_input_len))\n",
    "            clear_memory()\n",
    "\n",
    "        attribution_matrix = np.array(attribution_matrix)\n",
    "        attribution_matrix = np.abs(attribution_matrix)\n",
    "\n",
    "        # 零化'.'token的归因\n",
    "        dot_positions = [i for i, token in enumerate(input_tokens[:max_input_len]) if token == '.']\n",
    "        if dot_positions:\n",
    "            attribution_matrix[:, dot_positions] = 0.0\n",
    "\n",
    "        # 归一化\n",
    "        if attribution_matrix.max() > 0:\n",
    "            attribution_matrix = attribution_matrix / attribution_matrix.max()\n",
    "\n",
    "        # 清理\n",
    "        del input_encoding, target_encoding, input_ids, target_ids, input_embeds, baseline_embeds, decoder_input_ids\n",
    "        clear_memory()\n",
    "\n",
    "        return attribution_matrix, input_tokens, target_tokens\n",
    "\n",
    "    def create_sample_directory(self, sample_name: str) -> str:\n",
    "        \"\"\"创建样本目录，使用reactant名称而非序号\"\"\"\n",
    "        # 将SMILES中的特殊字符替换为下划线\n",
    "        safe_name = re.sub(r'[<>:\"/\\\\|?*]', '_', sample_name)[:50]  # 限制长度\n",
    "        sample_dir = os.path.join(self.output_dir, f\"reactant_{safe_name}\")\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "        return sample_dir\n",
    "\n",
    "    def create_visualizations_from_attributions(self, attributions: np.ndarray, input_tokens: List[str],\n",
    "                                              target_tokens: List[str], reactant_smiles: str) -> np.ndarray:\n",
    "        \"\"\"创建可视化图表\"\"\"\n",
    "        # 过滤token\n",
    "        input_display = self.filter_tokens(input_tokens)\n",
    "        target_display = self.filter_tokens(target_tokens)\n",
    "\n",
    "        input_mask = [i for i, token in enumerate(input_tokens) if token in input_display]\n",
    "        target_mask = [i for i, token in enumerate(target_tokens) if token in target_display]\n",
    "\n",
    "        if input_mask and target_mask:\n",
    "            attr_display = attributions[np.ix_(target_mask, input_mask)]\n",
    "        else:\n",
    "            attr_display = attributions\n",
    "\n",
    "        sample_dir = self.create_sample_directory(reactant_smiles)\n",
    "\n",
    "        # 1. Token Attribution Heatmap\n",
    "        self._create_heatmap(attr_display, input_display, target_display, sample_dir)\n",
    "\n",
    "        # 2. Global Attribution Plot\n",
    "        global_attr = self._create_global_plot(attributions, input_tokens, input_display, input_mask, sample_dir)\n",
    "\n",
    "        return global_attr\n",
    "\n",
    "    def _create_heatmap(self, attributions: np.ndarray, input_tokens: List[str],\n",
    "                       target_tokens: List[str], sample_dir: str):\n",
    "        \"\"\"创建热力图\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(max(16, len(input_tokens) * 0.8),\n",
    "                                       max(12, len(target_tokens) * 0.8)))\n",
    "\n",
    "        # 归一化\n",
    "        attr_norm = attributions\n",
    "        if attributions.max() > attributions.min():\n",
    "            attr_norm = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n",
    "\n",
    "        # 创建colormap\n",
    "        from matplotlib.colors import LinearSegmentedColormap\n",
    "        colors = ['#F1FAEE', COLORS['light_blue'], COLORS['primary'], COLORS['dark_blue']]\n",
    "        cmap = LinearSegmentedColormap.from_list('custom_blues', colors, N=100)\n",
    "\n",
    "        im = ax.imshow(attr_norm, cmap=cmap, aspect='auto', interpolation='nearest')\n",
    "\n",
    "        # 设置标签\n",
    "        ax.set_xticks(range(len(input_tokens)))\n",
    "        ax.set_xticklabels(input_tokens, rotation=45, ha='right', fontsize=35, color='black')\n",
    "        ax.set_yticks(range(len(target_tokens)))\n",
    "        ax.set_yticklabels(target_tokens, fontsize=35, color='black')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=35, length=8, width=1.5)\n",
    "\n",
    "        # 添加数值\n",
    "        for i in range(len(target_tokens)):\n",
    "            for j in range(len(input_tokens)):\n",
    "                if j < attributions.shape[1] and i < attributions.shape[0]:\n",
    "                    text_color = 'white' if attr_norm[i, j] > 0.6 else COLORS['dark_blue']\n",
    "                    ax.text(j, i, f'{attributions[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=text_color, fontsize=45, fontweight='bold')\n",
    "\n",
    "        ax.set_xlabel('Input Tokens (Reactant)', fontweight='bold', fontsize=36, color=COLORS['dark_blue'])\n",
    "        ax.set_ylabel('Output Tokens (Product)', fontweight='bold', fontsize=36, color=COLORS['dark_blue'])\n",
    "        ax.set_title('Token-to-Token Attribution Heatmap',\n",
    "                    fontweight='bold', pad=24, fontsize=36, color=COLORS['dark_blue'])\n",
    "\n",
    "        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "        cbar.set_label('Attribution Score', fontweight='bold', fontsize=36, color=COLORS['dark_blue'])\n",
    "        cbar.ax.tick_params(labelsize=26, width=1.5, length=6, colors=COLORS['dark_blue'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(sample_dir, \"token_attribution_heatmap.png\"), dpi=600, bbox_inches='tight', facecolor='white')\n",
    "        plt.close(fig)\n",
    "        del fig, ax, attr_norm, im\n",
    "        clear_memory()\n",
    "\n",
    "    def _create_global_plot(self, attributions: np.ndarray, input_tokens: List[str],\n",
    "                           input_display: List[str], input_mask: List[int], sample_dir: str) -> np.ndarray:\n",
    "        \"\"\"创建全局归因图，包含\".\"token并设置其score为0\"\"\"\n",
    "        global_attr_full = np.mean(np.abs(attributions), axis=0)\n",
    "\n",
    "        # 零化'.'token的归因\n",
    "        dot_positions = [i for i, token in enumerate(input_tokens) if token == '.']\n",
    "        if dot_positions:\n",
    "            global_attr_full[dot_positions] = 0.0\n",
    "\n",
    "        # 创建包含\".\"token的显示列表\n",
    "        input_display_with_dots = []\n",
    "        global_attr_with_dots = []\n",
    "\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            if token == '.' or token in input_display:\n",
    "                input_display_with_dots.append(token)\n",
    "                if i < len(global_attr_full):\n",
    "                    if token == '.':\n",
    "                        global_attr_with_dots.append(0.0)  # 确保\".\"的score为0\n",
    "                    else:\n",
    "                        global_attr_with_dots.append(global_attr_full[i])\n",
    "                else:\n",
    "                    global_attr_with_dots.append(0.0)\n",
    "\n",
    "        global_attr_with_dots = np.array(global_attr_with_dots)\n",
    "\n",
    "        print(f\"\\n--- GLOBAL ATTRIBUTION SCORES (INCLUDING DOTS) ---\")\n",
    "        print(\"Input tokens and their global attribution scores:\")\n",
    "        for i, (token, score) in enumerate(zip(input_display_with_dots, global_attr_with_dots)):\n",
    "            print(f\"  Token[{i}]: '{token}' -> Global Attribution: {score:.6f}\")\n",
    "        print(f\"{'='*40}\\n\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(max(14, len(input_display_with_dots) * 0.9), 7))\n",
    "\n",
    "        if len(global_attr_with_dots) > 0 and global_attr_with_dots.max() > 0:\n",
    "            normalized_values = global_attr_with_dots / global_attr_with_dots.max()\n",
    "            bar_colors = []\n",
    "            for i, (token, val) in enumerate(zip(input_display_with_dots, normalized_values)):\n",
    "                if token == '.':\n",
    "                    bar_colors.append(COLORS['neutral'])  # \".\"用灰色\n",
    "                else:\n",
    "                    bar_colors.append(plt.cm.Blues(0.3 + 0.7 * val))\n",
    "        else:\n",
    "            bar_colors = []\n",
    "            for token in input_display_with_dots:\n",
    "                if token == '.':\n",
    "                    bar_colors.append(COLORS['neutral'])\n",
    "                else:\n",
    "                    bar_colors.append(COLORS['light_blue'])\n",
    "\n",
    "        bars = ax.bar(range(len(input_display_with_dots)), global_attr_with_dots,\n",
    "                     color=bar_colors, alpha=0.9, edgecolor=COLORS['dark_blue'], linewidth=1.5)\n",
    "\n",
    "        # 添加数值标签\n",
    "        for bar, attr, token in zip(bars, global_attr_with_dots, input_display_with_dots):\n",
    "            height = bar.get_height()\n",
    "            if token == '.':\n",
    "                # 为\".\"添加特殊标记\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + max(height * 0.03, 0.01),\n",
    "                       '0.000\\n(separator)', ha='center', va='bottom', fontweight='bold',\n",
    "                       fontsize=16, color=COLORS['neutral'])\n",
    "            else:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + max(height * 0.03, 0.01),\n",
    "                       f'{attr:.3f}', ha='center', va='bottom', fontweight='bold',\n",
    "                       fontsize=18, color=COLORS['dark_blue'])\n",
    "\n",
    "        ax.set_xlabel('Input Tokens (Reactant)', fontweight='bold', fontsize=28, color=COLORS['dark_blue'])\n",
    "        ax.set_ylabel('Global Attribution Score', fontweight='bold', fontsize=28, color=COLORS['dark_blue'])\n",
    "        ax.set_title('Global Token Attribution (Including Separators)', fontweight='bold', pad=24, fontsize=30, color=COLORS['dark_blue'])\n",
    "        ax.set_xticks(range(len(input_display_with_dots)))\n",
    "        ax.set_xticklabels(input_display_with_dots, rotation=45, ha='right', fontsize=26, color='black')\n",
    "        ax.tick_params(axis='y', labelsize=20, width=1.5, length=8, colors=COLORS['dark_blue'])\n",
    "\n",
    "        for spine in ['bottom', 'left']:\n",
    "            ax.spines[spine].set_color(COLORS['dark_blue'])\n",
    "        for spine in ['top', 'right']:\n",
    "            ax.spines[spine].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(sample_dir, \"global_attribution.png\"), dpi=600, bbox_inches='tight', facecolor='white')\n",
    "        plt.close(fig)\n",
    "        del fig, ax, bars\n",
    "        clear_memory()\n",
    "\n",
    "        return global_attr_full\n",
    "\n",
    "    def create_molecule_image(self, smiles: str, size=(700, 500)) -> Image.Image:\n",
    "        \"\"\"创建分子图像\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return self._create_error_image(smiles, size)\n",
    "\n",
    "        try:\n",
    "            drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1])\n",
    "            drawer.SetFontSize(12)\n",
    "            drawer.drawOptions().addAtomIndices = False\n",
    "            drawer.DrawMolecule(mol)\n",
    "            drawer.FinishDrawing()\n",
    "            png_data = drawer.GetDrawingText()\n",
    "            img = Image.open(io.BytesIO(png_data))\n",
    "            del mol, drawer, png_data\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create molecule image: {e}\")\n",
    "            if mol:\n",
    "                del mol\n",
    "            return self._create_error_image(smiles, size)\n",
    "\n",
    "    def _create_error_image(self, smiles: str, size=(600, 400)) -> Image.Image:\n",
    "        \"\"\"创建错误SMILES图像\"\"\"\n",
    "        img = Image.new('RGB', size, 'white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # 绘制边框\n",
    "        draw.rectangle([1, 1, size[0]-2, size[1]-2], outline=COLORS['error'], width=3)\n",
    "\n",
    "        # 添加文字\n",
    "        center_x, center_y = size[0] // 2, size[1] // 2\n",
    "\n",
    "        font_large = get_font(28)\n",
    "        title = \"INVALID SMILES\"\n",
    "        title_bbox = draw.textbbox((0, 0), title, font=font_large)\n",
    "        title_width = title_bbox[2] - title_bbox[0]\n",
    "        draw.text((center_x - title_width//2, center_y - 80), title, fill=COLORS['error'], font=font_large)\n",
    "\n",
    "        font_small = get_font(24)\n",
    "        formatted_smiles = format_smiles_multiline(smiles, 30)\n",
    "        lines = formatted_smiles.split('\\n')\n",
    "\n",
    "        start_y = center_y - len(lines) * 15 + 30\n",
    "        for i, line in enumerate(lines):\n",
    "            line_bbox = draw.textbbox((0, 0), line, font=font_small)\n",
    "            line_width = line_bbox[2] - line_bbox[0]\n",
    "            draw.text((center_x - line_width//2, start_y + i * 30), line, fill=COLORS['dark_blue'], font=font_small)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def create_highlighted_molecule_image(self, smiles: str, global_attributions: np.ndarray,\n",
    "                                        input_tokens: List[str], size=(800, 600)) -> Image.Image:\n",
    "        \"\"\"创建高亮分子图像\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return self._create_error_image(smiles, size)\n",
    "\n",
    "        try:\n",
    "            # 过滤掉\".\"token，因为它不参与分子结构映射\n",
    "            input_display = self.filter_tokens(input_tokens)\n",
    "\n",
    "            # 创建原始token到过滤后token的映射（排除\".\"）\n",
    "            token_to_filtered_map = {}\n",
    "            filtered_index = 0\n",
    "            valid_attributions = []\n",
    "\n",
    "            for i, token in enumerate(input_tokens):\n",
    "                if token in input_display:  # 已经排除了\".\"\n",
    "                    token_to_filtered_map[i] = filtered_index\n",
    "                    if i < len(global_attributions):\n",
    "                        valid_attributions.append(global_attributions[i])\n",
    "                    else:\n",
    "                        valid_attributions.append(0.0)\n",
    "                    filtered_index += 1\n",
    "\n",
    "            valid_attributions = np.array(valid_attributions)\n",
    "\n",
    "            # 映射token到原子\n",
    "            atom_contributions = self.map_tokens_to_atoms(smiles, input_display, valid_attributions)\n",
    "\n",
    "            # 创建原子高亮和化学键高亮\n",
    "            atom_highlights = {}\n",
    "            bond_highlights = {}\n",
    "            num_atoms = mol.GetNumAtoms()\n",
    "\n",
    "            print(f\"\\n--- ATOM AND BOND HIGHLIGHTING DETAILS ---\")\n",
    "            if atom_contributions:\n",
    "                max_contribution = max(atom_contributions.values()) or 1.0\n",
    "                print(f\"Maximum contribution value: {max_contribution:.6f}\")\n",
    "                print(f\"Atom highlighting details:\")\n",
    "\n",
    "                # 计算原子颜色\n",
    "                for atom_idx in range(num_atoms):\n",
    "                    contribution = atom_contributions.get(atom_idx, 0.0)\n",
    "                    normalized_contribution = contribution / max_contribution\n",
    "                    intensity = 0.2 + 0.8 * normalized_contribution\n",
    "\n",
    "                    from matplotlib.colors import LinearSegmentedColormap\n",
    "                    cmap = LinearSegmentedColormap.from_list('custom_blues',\n",
    "                                                           [COLORS['background'], COLORS['dark_blue']])\n",
    "                    rgba = cmap(intensity)\n",
    "                    rgb = tuple(rgba[:3])\n",
    "                    atom_highlights[atom_idx] = rgb\n",
    "\n",
    "                    atom_symbol = mol.GetAtoms()[atom_idx].GetSymbol()\n",
    "                    print(f\"  Atom[{atom_idx}] ({atom_symbol}): contribution={contribution:.6f}, \"\n",
    "                          f\"normalized={normalized_contribution:.3f}, intensity={intensity:.3f}, \"\n",
    "                          f\"RGB=({rgb[0]:.3f}, {rgb[1]:.3f}, {rgb[2]:.3f})\")\n",
    "\n",
    "                # 计算化学键颜色（取连接原子的平均归因值）\n",
    "                print(f\"\\nBond highlighting details:\")\n",
    "                bond_list = []\n",
    "                for bond in mol.GetBonds():\n",
    "                    bond_idx = bond.GetIdx()\n",
    "                    atom1_idx = bond.GetBeginAtomIdx()\n",
    "                    atom2_idx = bond.GetEndAtomIdx()\n",
    "\n",
    "                    # 获取两个原子的归因值\n",
    "                    contrib1 = atom_contributions.get(atom1_idx, 0.0)\n",
    "                    contrib2 = atom_contributions.get(atom2_idx, 0.0)\n",
    "\n",
    "                    # 计算平均值作为化学键的归因值\n",
    "                    bond_contribution = (contrib1 + contrib2) / 2.0\n",
    "                    normalized_bond_contribution = bond_contribution / max_contribution\n",
    "                    bond_intensity = 0.2 + 0.8 * normalized_bond_contribution\n",
    "\n",
    "                    rgba = cmap(bond_intensity)\n",
    "                    rgb = tuple(rgba[:3])\n",
    "                    bond_highlights[bond_idx] = rgb\n",
    "                    bond_list.append(bond_idx)\n",
    "\n",
    "                    atom1_symbol = mol.GetAtoms()[atom1_idx].GetSymbol()\n",
    "                    atom2_symbol = mol.GetAtoms()[atom2_idx].GetSymbol()\n",
    "                    print(f\"  Bond[{bond_idx}] ({atom1_symbol}{atom1_idx}-{atom2_symbol}{atom2_idx}): \"\n",
    "                          f\"atom1_contrib={contrib1:.6f}, atom2_contrib={contrib2:.6f}, \"\n",
    "                          f\"avg_contrib={bond_contribution:.6f}, intensity={bond_intensity:.3f}, \"\n",
    "                          f\"RGB=({rgb[0]:.3f}, {rgb[1]:.3f}, {rgb[2]:.3f})\")\n",
    "\n",
    "            else:\n",
    "                print(\"No atom contributions found - using default highlighting\")\n",
    "                for atom_idx in range(num_atoms):\n",
    "                    atom_highlights[atom_idx] = (0.8, 0.9, 1.0)\n",
    "\n",
    "                bond_list = []\n",
    "                for bond in mol.GetBonds():\n",
    "                    bond_idx = bond.GetIdx()\n",
    "                    bond_highlights[bond_idx] = (0.8, 0.9, 1.0)\n",
    "                    bond_list.append(bond_idx)\n",
    "\n",
    "            print(f\"{'='*40}\\n\")\n",
    "\n",
    "            # 绘制分子\n",
    "            drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1])\n",
    "            drawer.SetFontSize(10)\n",
    "            drawer.drawOptions().addAtomIndices = False\n",
    "\n",
    "            # 同时高亮原子和化学键\n",
    "            if atom_highlights and bond_highlights:\n",
    "                drawer.DrawMolecule(mol,\n",
    "                                  highlightAtoms=list(atom_highlights.keys()),\n",
    "                                  highlightAtomColors=atom_highlights,\n",
    "                                  highlightBonds=bond_list,\n",
    "                                  highlightBondColors=bond_highlights)\n",
    "            elif atom_highlights:\n",
    "                drawer.DrawMolecule(mol, highlightAtoms=list(atom_highlights.keys()),\n",
    "                                  highlightAtomColors=atom_highlights)\n",
    "            else:\n",
    "                drawer.DrawMolecule(mol)\n",
    "\n",
    "            drawer.FinishDrawing()\n",
    "            png_data = drawer.GetDrawingText()\n",
    "            img = Image.open(io.BytesIO(png_data))\n",
    "\n",
    "            # 添加统计信息\n",
    "            new_img = Image.new('RGB', (size[0], size[1] + 60), 'white')\n",
    "            new_img.paste(img, (0, 0))\n",
    "\n",
    "            del mol, drawer, png_data, img\n",
    "            return new_img\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create highlighted molecule: {e}\")\n",
    "            if mol:\n",
    "                del mol\n",
    "            return self.create_molecule_image(smiles, size)\n",
    "\n",
    "    def _create_prediction_image(self, pred: Dict, true_products: Set[str], size=(550, 320)) -> Image.Image:\n",
    "        \"\"\"创建单个预测图像\"\"\"\n",
    "        if pred['is_valid'] and pred['sanitized_smiles']:\n",
    "            mol_img = self.create_molecule_image(pred['sanitized_smiles'], (550, 180))\n",
    "            img = Image.new('RGB', size, 'white')\n",
    "            img.paste(mol_img, (0, 0))\n",
    "\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            font = get_font(26)\n",
    "            font_small = get_font(22)\n",
    "\n",
    "            is_correct = pred['sanitized_smiles'] in true_products\n",
    "            status = f\"Rank {pred['rank']}: {'CORRECT' if is_correct else 'Valid'}\"\n",
    "            color = COLORS['correct_highlight'] if is_correct else COLORS['primary']\n",
    "\n",
    "            draw.text((8, 190), status, fill=color, font=font)\n",
    "\n",
    "            formatted_smiles = format_smiles_multiline(pred['sanitized_smiles'], 35)\n",
    "            for j, line in enumerate(formatted_smiles.split('\\n')):\n",
    "                text = f\"{'SMILES: ' if j == 0 else '        '}{line}\"\n",
    "                draw.text((8, 220 + j * 25), text, fill=COLORS['dark_blue'], font=font_small)\n",
    "\n",
    "            del mol_img\n",
    "        else:\n",
    "            error_img = self._create_error_image(pred['raw_smiles'], (550, 180))\n",
    "            img = Image.new('RGB', size, 'white')\n",
    "            img.paste(error_img, (0, 0))\n",
    "\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            font = get_font(26)\n",
    "            font_small = get_font(22)\n",
    "\n",
    "            draw.text((8, 190), f\"Rank {pred['rank']}: Invalid\", fill=COLORS['error'], font=font)\n",
    "\n",
    "            formatted_smiles = format_smiles_multiline(pred['raw_smiles'], 35)\n",
    "            for j, line in enumerate(formatted_smiles.split('\\n')):\n",
    "                text = f\"{'SMILES: ' if j == 0 else '        '}{line}\"\n",
    "                draw.text((8, 220 + j * 25), text, fill=COLORS['error'], font=font_small)\n",
    "\n",
    "            del error_img\n",
    "\n",
    "        return img\n",
    "\n",
    "    def create_reaction_comparison_plot(self, reactant_smiles: str, predictions: List[Dict],\n",
    "                                      true_products: Set[str], global_attributions: np.ndarray,\n",
    "                                      input_tokens: List[str], reaction_type: str = \"Unknown\"):\n",
    "        \"\"\"创建反应对比图\"\"\"\n",
    "        print(f\"Creating comparison plot for reactant: {reactant_smiles[:50]}...\")\n",
    "\n",
    "        sample_dir = self.create_sample_directory(reactant_smiles)\n",
    "\n",
    "        fig = plt.figure(figsize=(28, 16))\n",
    "        gs = fig.add_gridspec(3, 3, height_ratios=[0.5, 4, 0.3], width_ratios=[1, 1, 1],\n",
    "                             hspace=0.12, wspace=0.08, left=0.03, right=0.97, top=0.95, bottom=0.05)\n",
    "\n",
    "        fig.suptitle('Chemical Reaction Prediction Analysis',\n",
    "                    fontweight='bold', fontsize=32, y=0.98, color=COLORS['dark_blue'])\n",
    "\n",
    "        # 第一列：Reactant\n",
    "        ax_reactant_title = fig.add_subplot(gs[0, 0])\n",
    "        formatted_reactant = format_smiles_multiline(reactant_smiles, 40)\n",
    "        ax_reactant_title.text(0.5, 0.5, f'Input Reactant\\n{formatted_reactant}',\n",
    "                              ha='center', va='center', transform=ax_reactant_title.transAxes,\n",
    "                              fontsize=22, fontweight='bold',\n",
    "                              bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=COLORS['light_blue'], alpha=0.8))\n",
    "        ax_reactant_title.axis('off')\n",
    "\n",
    "        ax_reactant_mol = fig.add_subplot(gs[1, 0])\n",
    "        highlighted_img = self.create_highlighted_molecule_image(\n",
    "            reactant_smiles, global_attributions, input_tokens, size=(700, 560)\n",
    "        )\n",
    "        ax_reactant_mol.imshow(highlighted_img)\n",
    "        ax_reactant_mol.set_title('Token-to-Token Highlighted Reactant\\n(Darker Blue = Higher Attribution)',\n",
    "                                 fontweight='bold', fontsize=26, pad=12, color=COLORS['dark_blue'])\n",
    "        ax_reactant_mol.axis('off')\n",
    "\n",
    "        # 第二列：Predictions\n",
    "        ax_pred_title = fig.add_subplot(gs[0, 1])\n",
    "        ax_pred_title.text(0.5, 0.5, 'Model Predictions (Top 3)', ha='center', va='center',\n",
    "                          transform=ax_pred_title.transAxes, fontsize=22, fontweight='bold',\n",
    "                          bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=COLORS['background'], alpha=0.8))\n",
    "        ax_pred_title.axis('off')\n",
    "\n",
    "        ax_pred_mol = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "        if predictions:\n",
    "            pred_images = [self._create_prediction_image(pred, true_products) for pred in predictions[:3]]\n",
    "            if pred_images:\n",
    "                combined_img = self._combine_images_vertically(pred_images)\n",
    "                ax_pred_mol.imshow(combined_img)\n",
    "                del pred_images, combined_img\n",
    "        else:\n",
    "            ax_pred_mol.text(0.5, 0.5, 'No predictions available', ha='center', va='center',\n",
    "                            transform=ax_pred_mol.transAxes, fontsize=20, fontweight='bold',\n",
    "                            color=COLORS['error'])\n",
    "\n",
    "        ax_pred_mol.set_title('Predicted Products', fontweight='bold', fontsize=26, pad=12, color=COLORS['dark_blue'])\n",
    "        ax_pred_mol.axis('off')\n",
    "\n",
    "        # 第三列：Ground Truth\n",
    "        ax_true_title = fig.add_subplot(gs[0, 2])\n",
    "        ax_true_title.text(0.5, 0.5, f'Ground Truth Products\\n({len(true_products)} total)',\n",
    "                          ha='center', va='center', transform=ax_true_title.transAxes,\n",
    "                          fontsize=22, fontweight='bold',\n",
    "                          bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=COLORS['success'], alpha=0.3))\n",
    "        ax_true_title.axis('off')\n",
    "\n",
    "        ax_true_mol = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "        if true_products:\n",
    "            true_images = []\n",
    "            for i, product in enumerate(sorted(list(true_products))[:3]):\n",
    "                mol_img = self.create_molecule_image(product, (550, 180))\n",
    "                img = Image.new('RGB', (550, 320), 'white')\n",
    "                img.paste(mol_img, (0, 0))\n",
    "\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                font = get_font(26)\n",
    "                font_small = get_font(22)\n",
    "\n",
    "                draw.text((8, 190), f\"True Product {i+1}\", fill=COLORS['success'], font=font)\n",
    "\n",
    "                formatted_smiles = format_smiles_multiline(product, 35)\n",
    "                for j, line in enumerate(formatted_smiles.split('\\n')):\n",
    "                    text = f\"{'SMILES: ' if j == 0 else '        '}{line}\"\n",
    "                    draw.text((8, 220 + j * 25), text, fill=COLORS['dark_blue'], font=font_small)\n",
    "\n",
    "                true_images.append(img)\n",
    "                del mol_img\n",
    "\n",
    "            if true_images:\n",
    "                combined_img = self._combine_images_vertically(true_images)\n",
    "                ax_true_mol.imshow(combined_img)\n",
    "                del true_images, combined_img\n",
    "        else:\n",
    "            ax_true_mol.text(0.5, 0.5, 'No ground truth\\nproducts available',\n",
    "                            ha='center', va='center', transform=ax_true_mol.transAxes,\n",
    "                            fontsize=20, fontweight='bold', color=COLORS['neutral'])\n",
    "\n",
    "        ax_true_mol.set_title('Ground Truth Products', fontweight='bold', fontsize=26, pad=12, color=COLORS['dark_blue'])\n",
    "        ax_true_mol.axis('off')\n",
    "\n",
    "        # 底部统计\n",
    "        ax_stats = fig.add_subplot(gs[2, :])\n",
    "        valid_count = sum(1 for p in predictions if p['is_valid'])\n",
    "        correct_count = sum(1 for p in predictions if p['is_valid'] and p['sanitized_smiles'] in true_products)\n",
    "\n",
    "        stats_text = (f\"Performance: Valid ({valid_count}/{len(predictions)}) | \"\n",
    "                     f\"Correct ({correct_count}/{len(predictions)}) | \"\n",
    "                     f\"Reaction Type: {reaction_type}\")\n",
    "\n",
    "        ax_stats.text(0.5, 0.5, stats_text, ha='center', va='center',\n",
    "                     transform=ax_stats.transAxes, fontsize=24, fontweight='bold',\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=COLORS['background'], alpha=0.8),\n",
    "                     color=COLORS['dark_blue'])\n",
    "        ax_stats.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(sample_dir, \"reaction_comparison.png\")\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight', facecolor='white', pad_inches=0.03)\n",
    "        plt.close(fig)\n",
    "\n",
    "        del fig, highlighted_img\n",
    "        clear_memory()\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "    def _combine_images_vertically(self, images: List[Image.Image]) -> Image.Image:\n",
    "        \"\"\"垂直组合图像\"\"\"\n",
    "        total_height = sum(img.height for img in images)\n",
    "        max_width = max(img.width for img in images)\n",
    "        combined = Image.new('RGB', (max_width, total_height), 'white')\n",
    "\n",
    "        y_offset = 0\n",
    "        for img in images:\n",
    "            combined.paste(img, (0, y_offset))\n",
    "            y_offset += img.height\n",
    "            del img\n",
    "\n",
    "        return combined\n",
    "\n",
    "    def analyze_reaction(self, reactant_smiles: str, true_products: Set[str],\n",
    "                        reaction_type: str = \"Unknown\") -> Dict:\n",
    "        \"\"\"分析反应\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing reactant: {reactant_smiles}\")\n",
    "        print(f\"Reaction type: {reaction_type}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # 预测\n",
    "        predictions = self.beam_search_predict(reactant_smiles)\n",
    "        best_pred = next((p for p in predictions if p['is_valid']), None)\n",
    "        target_smiles = best_pred['sanitized_smiles'] if best_pred else (predictions[0]['raw_smiles'] if predictions else reactant_smiles)\n",
    "\n",
    "        print(f\"Using target: {target_smiles}\")\n",
    "\n",
    "        # 计算归因\n",
    "        attributions, input_tokens, target_tokens = self.compute_attribution_scores(reactant_smiles, target_smiles)\n",
    "\n",
    "        # 创建可视化\n",
    "        global_attr = self.create_visualizations_from_attributions(\n",
    "            attributions, input_tokens, target_tokens, reactant_smiles\n",
    "        )\n",
    "\n",
    "        # 创建对比图\n",
    "        self.create_reaction_comparison_plot(\n",
    "            reactant_smiles, predictions, true_products, global_attr, input_tokens, reaction_type\n",
    "        )\n",
    "\n",
    "        clear_memory()\n",
    "\n",
    "        return {\n",
    "            'reactant': reactant_smiles, 'predictions': predictions, 'true_products': true_products,\n",
    "            'best_prediction': best_pred, 'attributions': attributions,\n",
    "            'global_attributions': global_attr, 'input_tokens': input_tokens,\n",
    "            'target_tokens': target_tokens, 'reaction_type': reaction_type, 'analysis_complete': True\n",
    "        }\n",
    "\n",
    "def main_analysis(model, tokenizer, device, data_path, dataset_split, reactant_smiles):\n",
    "    \"\"\"主分析函数 - 手动输入reactant进行分析\"\"\"\n",
    "    print(\"Initializing Chemical Reaction Explainer...\")\n",
    "    explainer = ChemicalReactionExplainer(model, tokenizer, device)\n",
    "\n",
    "    print(f\"Loading data from '{data_path}' using {dataset_split} split...\")\n",
    "    try:\n",
    "        dataset = load_from_disk(data_path)[dataset_split]\n",
    "        products_map, types_map = group_data_mappings(dataset)\n",
    "        print(f\"Loaded {len(dataset)} samples from {dataset_split} split, {len(products_map)} unique reactants\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "\n",
    "    # 检查输入的reactant是否存在于数据集中\n",
    "    if reactant_smiles not in products_map:\n",
    "        print(f\"Warning: Reactant '{reactant_smiles}' not found in the {dataset_split} dataset.\")\n",
    "        print(\"Available reactants (first 10):\")\n",
    "        for i, reactant in enumerate(list(products_map.keys())[:10]):\n",
    "            print(f\"  {i+1}: {reactant}\")\n",
    "        print(f\"Total available reactants: {len(products_map)}\")\n",
    "\n",
    "        # 选择是否继续分析\n",
    "        user_choice = input(\"Continue analysis anyway? (y/n): \").strip().lower()\n",
    "        if user_choice != 'y':\n",
    "            return\n",
    "\n",
    "        # 使用空的产品集合\n",
    "        true_products = set()\n",
    "        reaction_type = \"Unknown\"\n",
    "    else:\n",
    "        true_products = products_map[reactant_smiles]\n",
    "        reaction_type = types_map.get(reactant_smiles, \"Unknown\")\n",
    "        print(f\"Found reactant in dataset with {len(true_products)} product(s) and reaction type: {reaction_type}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nStarting analysis for reactant: {reactant_smiles}\")\n",
    "        result = explainer.analyze_reaction(reactant_smiles, true_products, reaction_type)\n",
    "        print(f\"✓ Analysis completed successfully\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis Summary:\")\n",
    "        print(f\"Reactant: {reactant_smiles}\")\n",
    "        print(f\"Reaction Type: {reaction_type}\")\n",
    "        print(f\"True Products: {len(true_products)}\")\n",
    "        print(f\"Valid Predictions: {sum(1 for p in result['predictions'] if p['is_valid'])}\")\n",
    "        print(f\"Results saved in: {explainer.output_dir}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during analysis: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        clear_memory()\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_MODEL_NAME = \"google/flan-t5-base\"\n",
    "    ADAPTER_MODEL_PATH = \"./best_model_multi_eval_v3_correct_loss/\"\n",
    "    DATA_PATH = './data/data'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 加载模型\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    from peft import PeftModel\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        BASE_MODEL_NAME, quantization_config=quantization_config, device_map={\"\": DEVICE}\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_MODEL_PATH)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_PATH)\n",
    "\n",
    "    # 配置模型\n",
    "    for attr in ['bos_token_id', 'eos_token_id', 'pad_token_id', 'decoder_start_token_id']:\n",
    "        setattr(model.config, attr, getattr(tokenizer, attr.replace('decoder_start_', 'pad_')))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 手动配置参数\n",
    "    DATASET_SPLIT = \"test\"  # 可选: \"train\", \"test\", \"validation\"\n",
    "    REACTANT_SMILES = \"C=CC(C)C.[OH]\"  # 手动输入要分析的反应物SMILES\n",
    "\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Dataset Split: {DATASET_SPLIT}\")\n",
    "    print(f\"  Reactant SMILES: {REACTANT_SMILES}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "\n",
    "    # 运行分析\n",
    "    result = main_analysis(model, tokenizer, DEVICE, DATA_PATH, DATASET_SPLIT, REACTANT_SMILES)\n",
    "\n",
    "    if result:\n",
    "        print(\"Analysis completed successfully!\")\n",
    "    else:\n",
    "        print(\"Analysis failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
